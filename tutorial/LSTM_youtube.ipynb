{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_youtube.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7Y2uC3vaZtU"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5spTMKG_eOvR"
      },
      "source": [
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H40VzZ2barOR"
      },
      "source": [
        "leng=3\n",
        "\n",
        "data = [[i+j for j in range(leng)] for i in range(100)]\n",
        "data = np.array(data, dtype=np.float32)\n",
        "target = [[i+j+1 for j in range(leng)] for i in range(1,101)]\n",
        "target = np.array(target, dtype=np.float32)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hxNh-DRa_bR",
        "outputId": "cf8f1ac6-7f06-4525-8655-9dc2806cc3cb"
      },
      "source": [
        "data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0.,   1.,   2.],\n",
              "       [  1.,   2.,   3.],\n",
              "       [  2.,   3.,   4.],\n",
              "       [  3.,   4.,   5.],\n",
              "       [  4.,   5.,   6.],\n",
              "       [  5.,   6.,   7.],\n",
              "       [  6.,   7.,   8.],\n",
              "       [  7.,   8.,   9.],\n",
              "       [  8.,   9.,  10.],\n",
              "       [  9.,  10.,  11.],\n",
              "       [ 10.,  11.,  12.],\n",
              "       [ 11.,  12.,  13.],\n",
              "       [ 12.,  13.,  14.],\n",
              "       [ 13.,  14.,  15.],\n",
              "       [ 14.,  15.,  16.],\n",
              "       [ 15.,  16.,  17.],\n",
              "       [ 16.,  17.,  18.],\n",
              "       [ 17.,  18.,  19.],\n",
              "       [ 18.,  19.,  20.],\n",
              "       [ 19.,  20.,  21.],\n",
              "       [ 20.,  21.,  22.],\n",
              "       [ 21.,  22.,  23.],\n",
              "       [ 22.,  23.,  24.],\n",
              "       [ 23.,  24.,  25.],\n",
              "       [ 24.,  25.,  26.],\n",
              "       [ 25.,  26.,  27.],\n",
              "       [ 26.,  27.,  28.],\n",
              "       [ 27.,  28.,  29.],\n",
              "       [ 28.,  29.,  30.],\n",
              "       [ 29.,  30.,  31.],\n",
              "       [ 30.,  31.,  32.],\n",
              "       [ 31.,  32.,  33.],\n",
              "       [ 32.,  33.,  34.],\n",
              "       [ 33.,  34.,  35.],\n",
              "       [ 34.,  35.,  36.],\n",
              "       [ 35.,  36.,  37.],\n",
              "       [ 36.,  37.,  38.],\n",
              "       [ 37.,  38.,  39.],\n",
              "       [ 38.,  39.,  40.],\n",
              "       [ 39.,  40.,  41.],\n",
              "       [ 40.,  41.,  42.],\n",
              "       [ 41.,  42.,  43.],\n",
              "       [ 42.,  43.,  44.],\n",
              "       [ 43.,  44.,  45.],\n",
              "       [ 44.,  45.,  46.],\n",
              "       [ 45.,  46.,  47.],\n",
              "       [ 46.,  47.,  48.],\n",
              "       [ 47.,  48.,  49.],\n",
              "       [ 48.,  49.,  50.],\n",
              "       [ 49.,  50.,  51.],\n",
              "       [ 50.,  51.,  52.],\n",
              "       [ 51.,  52.,  53.],\n",
              "       [ 52.,  53.,  54.],\n",
              "       [ 53.,  54.,  55.],\n",
              "       [ 54.,  55.,  56.],\n",
              "       [ 55.,  56.,  57.],\n",
              "       [ 56.,  57.,  58.],\n",
              "       [ 57.,  58.,  59.],\n",
              "       [ 58.,  59.,  60.],\n",
              "       [ 59.,  60.,  61.],\n",
              "       [ 60.,  61.,  62.],\n",
              "       [ 61.,  62.,  63.],\n",
              "       [ 62.,  63.,  64.],\n",
              "       [ 63.,  64.,  65.],\n",
              "       [ 64.,  65.,  66.],\n",
              "       [ 65.,  66.,  67.],\n",
              "       [ 66.,  67.,  68.],\n",
              "       [ 67.,  68.,  69.],\n",
              "       [ 68.,  69.,  70.],\n",
              "       [ 69.,  70.,  71.],\n",
              "       [ 70.,  71.,  72.],\n",
              "       [ 71.,  72.,  73.],\n",
              "       [ 72.,  73.,  74.],\n",
              "       [ 73.,  74.,  75.],\n",
              "       [ 74.,  75.,  76.],\n",
              "       [ 75.,  76.,  77.],\n",
              "       [ 76.,  77.,  78.],\n",
              "       [ 77.,  78.,  79.],\n",
              "       [ 78.,  79.,  80.],\n",
              "       [ 79.,  80.,  81.],\n",
              "       [ 80.,  81.,  82.],\n",
              "       [ 81.,  82.,  83.],\n",
              "       [ 82.,  83.,  84.],\n",
              "       [ 83.,  84.,  85.],\n",
              "       [ 84.,  85.,  86.],\n",
              "       [ 85.,  86.,  87.],\n",
              "       [ 86.,  87.,  88.],\n",
              "       [ 87.,  88.,  89.],\n",
              "       [ 88.,  89.,  90.],\n",
              "       [ 89.,  90.,  91.],\n",
              "       [ 90.,  91.,  92.],\n",
              "       [ 91.,  92.,  93.],\n",
              "       [ 92.,  93.,  94.],\n",
              "       [ 93.,  94.,  95.],\n",
              "       [ 94.,  95.,  96.],\n",
              "       [ 95.,  96.,  97.],\n",
              "       [ 96.,  97.,  98.],\n",
              "       [ 97.,  98.,  99.],\n",
              "       [ 98.,  99., 100.],\n",
              "       [ 99., 100., 101.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXIrg_JpbAjx",
        "outputId": "a1e6b0d7-31ca-4ef6-a27c-3254af609640"
      },
      "source": [
        "target"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  2.,   3.,   4.],\n",
              "       [  3.,   4.,   5.],\n",
              "       [  4.,   5.,   6.],\n",
              "       [  5.,   6.,   7.],\n",
              "       [  6.,   7.,   8.],\n",
              "       [  7.,   8.,   9.],\n",
              "       [  8.,   9.,  10.],\n",
              "       [  9.,  10.,  11.],\n",
              "       [ 10.,  11.,  12.],\n",
              "       [ 11.,  12.,  13.],\n",
              "       [ 12.,  13.,  14.],\n",
              "       [ 13.,  14.,  15.],\n",
              "       [ 14.,  15.,  16.],\n",
              "       [ 15.,  16.,  17.],\n",
              "       [ 16.,  17.,  18.],\n",
              "       [ 17.,  18.,  19.],\n",
              "       [ 18.,  19.,  20.],\n",
              "       [ 19.,  20.,  21.],\n",
              "       [ 20.,  21.,  22.],\n",
              "       [ 21.,  22.,  23.],\n",
              "       [ 22.,  23.,  24.],\n",
              "       [ 23.,  24.,  25.],\n",
              "       [ 24.,  25.,  26.],\n",
              "       [ 25.,  26.,  27.],\n",
              "       [ 26.,  27.,  28.],\n",
              "       [ 27.,  28.,  29.],\n",
              "       [ 28.,  29.,  30.],\n",
              "       [ 29.,  30.,  31.],\n",
              "       [ 30.,  31.,  32.],\n",
              "       [ 31.,  32.,  33.],\n",
              "       [ 32.,  33.,  34.],\n",
              "       [ 33.,  34.,  35.],\n",
              "       [ 34.,  35.,  36.],\n",
              "       [ 35.,  36.,  37.],\n",
              "       [ 36.,  37.,  38.],\n",
              "       [ 37.,  38.,  39.],\n",
              "       [ 38.,  39.,  40.],\n",
              "       [ 39.,  40.,  41.],\n",
              "       [ 40.,  41.,  42.],\n",
              "       [ 41.,  42.,  43.],\n",
              "       [ 42.,  43.,  44.],\n",
              "       [ 43.,  44.,  45.],\n",
              "       [ 44.,  45.,  46.],\n",
              "       [ 45.,  46.,  47.],\n",
              "       [ 46.,  47.,  48.],\n",
              "       [ 47.,  48.,  49.],\n",
              "       [ 48.,  49.,  50.],\n",
              "       [ 49.,  50.,  51.],\n",
              "       [ 50.,  51.,  52.],\n",
              "       [ 51.,  52.,  53.],\n",
              "       [ 52.,  53.,  54.],\n",
              "       [ 53.,  54.,  55.],\n",
              "       [ 54.,  55.,  56.],\n",
              "       [ 55.,  56.,  57.],\n",
              "       [ 56.,  57.,  58.],\n",
              "       [ 57.,  58.,  59.],\n",
              "       [ 58.,  59.,  60.],\n",
              "       [ 59.,  60.,  61.],\n",
              "       [ 60.,  61.,  62.],\n",
              "       [ 61.,  62.,  63.],\n",
              "       [ 62.,  63.,  64.],\n",
              "       [ 63.,  64.,  65.],\n",
              "       [ 64.,  65.,  66.],\n",
              "       [ 65.,  66.,  67.],\n",
              "       [ 66.,  67.,  68.],\n",
              "       [ 67.,  68.,  69.],\n",
              "       [ 68.,  69.,  70.],\n",
              "       [ 69.,  70.,  71.],\n",
              "       [ 70.,  71.,  72.],\n",
              "       [ 71.,  72.,  73.],\n",
              "       [ 72.,  73.,  74.],\n",
              "       [ 73.,  74.,  75.],\n",
              "       [ 74.,  75.,  76.],\n",
              "       [ 75.,  76.,  77.],\n",
              "       [ 76.,  77.,  78.],\n",
              "       [ 77.,  78.,  79.],\n",
              "       [ 78.,  79.,  80.],\n",
              "       [ 79.,  80.,  81.],\n",
              "       [ 80.,  81.,  82.],\n",
              "       [ 81.,  82.,  83.],\n",
              "       [ 82.,  83.,  84.],\n",
              "       [ 83.,  84.,  85.],\n",
              "       [ 84.,  85.,  86.],\n",
              "       [ 85.,  86.,  87.],\n",
              "       [ 86.,  87.,  88.],\n",
              "       [ 87.,  88.,  89.],\n",
              "       [ 88.,  89.,  90.],\n",
              "       [ 89.,  90.,  91.],\n",
              "       [ 90.,  91.,  92.],\n",
              "       [ 91.,  92.,  93.],\n",
              "       [ 92.,  93.,  94.],\n",
              "       [ 93.,  94.,  95.],\n",
              "       [ 94.,  95.,  96.],\n",
              "       [ 95.,  96.,  97.],\n",
              "       [ 96.,  97.,  98.],\n",
              "       [ 97.,  98.,  99.],\n",
              "       [ 98.,  99., 100.],\n",
              "       [ 99., 100., 101.],\n",
              "       [100., 101., 102.],\n",
              "       [101., 102., 103.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-QuxKfsbDG5"
      },
      "source": [
        "# divided by 200 get normaliztion otherwse gardient value will be not good and prediction iwill be bad\n",
        "data = data.reshape(100, 1, leng)/200\n",
        "target = target.reshape(100,1,leng)/200"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FX-ewa4bIvZ"
      },
      "source": [
        " #Build Model\n",
        "# return sequence = true means each layer passes its value to the neaxt layer\n",
        "model = Sequential()  \n",
        "model.add(LSTM(leng, input_shape=(1, leng),return_sequences=True,activation='sigmoid'))\n",
        "model.add(LSTM(leng, input_shape=(1, leng),return_sequences=True,activation='sigmoid'))\n",
        "model.add(LSTM(leng, input_shape=(1, leng),return_sequences=True,activation='sigmoid'))\n",
        "model.add(LSTM(leng, input_shape=(1, leng),return_sequences=True,activation='sigmoid'))\n",
        "model.add(LSTM(leng, input_shape=(1, leng),return_sequences=True,activation='sigmoid'))\n",
        "model.add(LSTM(leng, input_shape=(1, leng),return_sequences=True,activation='sigmoid'))\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYUMoh5ygD5x"
      },
      "source": [
        "model.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8kAs8XNcV8r",
        "outputId": "0271a962-c63f-4879-8bca-8667a0056fe8"
      },
      "source": [
        "history=model.fit(data, target,epochs=100, batch_size=50,validation_data=(data,target))\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2/2 [==============================] - 7s 1s/step - loss: 0.0221 - accuracy: 0.0000e+00 - val_loss: 0.0220 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.0220 - accuracy: 0.0000e+00 - val_loss: 0.0220 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0220 - accuracy: 0.0000e+00 - val_loss: 0.0220 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.0220 - accuracy: 0.0000e+00 - val_loss: 0.0219 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0219 - accuracy: 0.0000e+00 - val_loss: 0.0219 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.0219 - accuracy: 0.0000e+00 - val_loss: 0.0219 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.0219 - accuracy: 0.0000e+00 - val_loss: 0.0218 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.0218 - accuracy: 0.0000e+00 - val_loss: 0.0218 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0218 - accuracy: 0.0000e+00 - val_loss: 0.0218 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0218 - accuracy: 0.0000e+00 - val_loss: 0.0217 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0217 - accuracy: 0.0000e+00 - val_loss: 0.0217 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0217 - accuracy: 0.0000e+00 - val_loss: 0.0217 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0217 - accuracy: 0.0000e+00 - val_loss: 0.0217 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.0217 - accuracy: 0.0000e+00 - val_loss: 0.0216 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0216 - accuracy: 0.0000e+00 - val_loss: 0.0216 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0216 - accuracy: 0.0000e+00 - val_loss: 0.0216 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0216 - accuracy: 0.0000e+00 - val_loss: 0.0215 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.0215 - accuracy: 0.0000e+00 - val_loss: 0.0215 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0215 - accuracy: 0.0000e+00 - val_loss: 0.0215 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0215 - accuracy: 0.0000e+00 - val_loss: 0.0215 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0215 - accuracy: 0.0000e+00 - val_loss: 0.0215 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.0215 - accuracy: 0.0000e+00 - val_loss: 0.0214 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.0214 - accuracy: 0.0000e+00 - val_loss: 0.0214 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0214 - accuracy: 0.0000e+00 - val_loss: 0.0214 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0214 - accuracy: 0.0000e+00 - val_loss: 0.0214 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0214 - accuracy: 0.0000e+00 - val_loss: 0.0214 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0214 - accuracy: 0.0000e+00 - val_loss: 0.0213 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0213 - accuracy: 0.0000e+00 - val_loss: 0.0213 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0213 - accuracy: 0.0000e+00 - val_loss: 0.0213 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0213 - accuracy: 0.0000e+00 - val_loss: 0.0213 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0213 - accuracy: 0.0000e+00 - val_loss: 0.0213 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0213 - accuracy: 0.0000e+00 - val_loss: 0.0213 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.0213 - accuracy: 0.0000e+00 - val_loss: 0.0212 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - val_loss: 0.0212 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - val_loss: 0.0212 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - val_loss: 0.0212 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - val_loss: 0.0212 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - val_loss: 0.0212 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - val_loss: 0.0212 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - val_loss: 0.0211 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - val_loss: 0.0211 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - val_loss: 0.0211 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - val_loss: 0.0211 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - val_loss: 0.0211 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - val_loss: 0.0211 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - val_loss: 0.0211 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - val_loss: 0.0211 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - val_loss: 0.0211 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - val_loss: 0.0211 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - val_loss: 0.0210 - val_accuracy: 0.0000e+00\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0210 - val_accuracy: 0.0000e+00\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0210 - val_accuracy: 0.0000e+00\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0210 - val_accuracy: 0.0000e+00\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0210 - val_accuracy: 0.0000e+00\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0210 - val_accuracy: 0.0000e+00\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0210 - val_accuracy: 0.0000e+00\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0210 - val_accuracy: 0.0000e+00\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0210 - val_accuracy: 0.0000e+00\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0210 - val_accuracy: 0.0000e+00\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0210 - val_accuracy: 0.0000e+00\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0210 - val_accuracy: 0.0000e+00\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0210 - val_accuracy: 0.0000e+00\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0210 - val_accuracy: 0.0000e+00\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0210 - val_accuracy: 0.0000e+00\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0210 - val_accuracy: 0.0000e+00\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZPMd6sFcebx",
        "outputId": "cde0f6c5-0a73-4ac6-865c-8b6a07629d58"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_24 (LSTM)               (None, 1, 3)              84        \n",
            "_________________________________________________________________\n",
            "lstm_25 (LSTM)               (None, 1, 3)              84        \n",
            "_________________________________________________________________\n",
            "lstm_26 (LSTM)               (None, 1, 3)              84        \n",
            "_________________________________________________________________\n",
            "lstm_27 (LSTM)               (None, 1, 3)              84        \n",
            "_________________________________________________________________\n",
            "lstm_28 (LSTM)               (None, 1, 3)              84        \n",
            "_________________________________________________________________\n",
            "lstm_29 (LSTM)               (None, 1, 3)              84        \n",
            "=================================================================\n",
            "Total params: 504\n",
            "Trainable params: 504\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31FBv8VWbLC5"
      },
      "source": [
        "results= model.predict(data)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "8iNEY0U8by7D",
        "outputId": "ffa066ba-4abc-4e4c-d7e2-32959735d8de"
      },
      "source": [
        "# to plt and see whether my prediction are good or not\n",
        "plt.scatter(range(20),results,c='r')\n",
        "plt.scatter(range(20),target,c='g')\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-4dc04d447d58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# to plt and see whether my prediction are good or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[1;32m   2814\u001b[0m         \u001b[0mverts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medgecolors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2815\u001b[0m         plotnonfinite=plotnonfinite, **({\"data\": data} if data is not\n\u001b[0;32m-> 2816\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2817\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2818\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4389\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4391\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must be the same size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must be the same size"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "2e8GTNjFeM4w",
        "outputId": "30d257da-e835-47ab-84e3-97b712c53035"
      },
      "source": [
        "# as we can see loss has not finally becoming flat therefore try to create more epochs or change hyperparamter\n",
        "plt.plot(history.history['loss'])\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vBA16yzfpXH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}